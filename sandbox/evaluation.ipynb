{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "willing-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blond-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inappropriate-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test = utils.read_current(\"../data/round_254/\")\n",
    "validation = test.query(\"data_type == 'validation'\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "industrial-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.Booster(model_file=\"../models/lgb001.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "moral-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [f for f in validation.columns if f.startswith(\"feature\")]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hungry-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation[\"prediction\"] = model.predict(validation[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "empirical-uruguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era\n",
      "era121    0.044188\n",
      "era122    0.019480\n",
      "era123    0.041258\n",
      "era124    0.062225\n",
      "era125    0.024602\n",
      "era126    0.043132\n",
      "era127    0.012770\n",
      "era128    0.060628\n",
      "era129   -0.017926\n",
      "era130    0.067782\n",
      "era131    0.035525\n",
      "era132    0.069603\n",
      "era197    0.031571\n",
      "era198    0.013926\n",
      "era199   -0.024592\n",
      "era200   -0.002004\n",
      "era201    0.006837\n",
      "era202    0.037300\n",
      "era203    0.031323\n",
      "era204    0.024924\n",
      "era205    0.003320\n",
      "era206   -0.012033\n",
      "era207    0.053210\n",
      "era208    0.053396\n",
      "era209    0.041266\n",
      "era210   -0.018317\n",
      "era211   -0.024732\n",
      "era212    0.022163\n",
      "dtype: float64\n",
      "Spearman Correlation: 0.025\n",
      "Average Payout: 0.1251\n",
      "Sharpe Ratio: 0.8868\n",
      "Mean Absolute Error (MAE): 0.1542\n",
      "Max drawdown: 0.08127724091443644\n",
      "Feature exposure: 0.08414838848691095, Max Feature Exposure: 0.3081406795169704, Square Sum: 2.404067154818816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def spearmanr(target, pred):\n",
    "    return np.corrcoef(target, pred.rank(pct=True, method=\"first\"))[0, 1]\n",
    "\n",
    "\n",
    "def sharpe_ratio(corrs: pd.Series) -> np.float32:\n",
    "    \"\"\"\n",
    "        Calculate the Sharpe ratio for Numerai by using grouped per-era data\n",
    "\n",
    "        :param corrs: A Pandas Series containing the Spearman correlations for each era\n",
    "        :return: A float denoting the Sharpe ratio of your predictions.\n",
    "        \"\"\"\n",
    "    return corrs.mean() / corrs.std()\n",
    "\n",
    "\n",
    "# https://parmarsuraj99.medium.com/evaluating-financial-machine-learning-models-on-numerai-3562da8fd90\n",
    "def calculate_feature_exposure(df, feature_names, prediction_name=\"prediction\") -> list:\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    -----\n",
    "    feature_exposure, max_feat_exposure, square_sum_feature_exposure = calculate_feature_exposure(df, feature_names)\n",
    "    \"\"\"\n",
    "\n",
    "    exposures = []\n",
    "    for feature_name in feature_names:\n",
    "        exposures.append(spearmanr(df[feature_name], df[prediction_name]))\n",
    "\n",
    "    max_feat_exposure = np.max(np.abs(exposures))\n",
    "    square_sum_feature_exposure = np.sum([e ** 2 for e in exposures])\n",
    "    feature_exposure = np.std(exposures)\n",
    "\n",
    "    return [feature_exposure, max_feat_exposure, square_sum_feature_exposure]\n",
    "\n",
    "\n",
    "# Calculating Max Drawdown\n",
    "def max_drawdown(df, prediction_name=\"prediction\", target_name=\"target\"):\n",
    "    scores_per_era = df.groupby(\"era\").apply(\n",
    "        lambda x: spearmanr(x[prediction_name], x[target_name])\n",
    "    )\n",
    "\n",
    "    rolling_max = (\n",
    "        (scores_per_era + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
    "    )\n",
    "    daily_value = (scores_per_era + 1).cumprod()\n",
    "    max_drawdown = (rolling_max - daily_value).max()\n",
    "\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def evaluate(df: pd.DataFrame, features) -> tuple:\n",
    "    \"\"\"\n",
    "        Evaluate and display relevant metrics for Numerai \n",
    "\n",
    "        :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and a column for predictions\n",
    "        :param pred_col: The column where the predictions are stored\n",
    "        :return: A tuple of float containing the metrics\n",
    "        \"\"\"\n",
    "\n",
    "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
    "        \"\"\"Calculates Spearman correlation\"\"\"\n",
    "        return spearmanr(sub_df[\"target\"], sub_df[\"prediction\"])\n",
    "\n",
    "    # Calculate metrics\n",
    "    corrs = df.groupby(\"era\").apply(_score)\n",
    "    print(corrs)\n",
    "    payout_raw = (corrs / 0.2).clip(-1, 1)\n",
    "    spearman = round(corrs.mean(), 4)\n",
    "\n",
    "    payout = round(payout_raw.mean(), 4)\n",
    "    numerai_sharpe = round(sharpe_ratio(corrs), 4)\n",
    "    mae = mean_absolute_error(df[\"target\"], df[\"prediction\"]).round(4)\n",
    "    drawdown = max_drawdown(df)\n",
    "    fe, max_fe, square_sum_fe = calculate_feature_exposure(df, features)\n",
    "\n",
    "    # Display metrics\n",
    "    print(f\"Spearman Correlation: {spearman}\")\n",
    "    print(f\"Average Payout: {payout}\")\n",
    "    print(f\"Sharpe Ratio: {numerai_sharpe}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Max drawdown: {drawdown}\")\n",
    "    print(\n",
    "        f\"Feature exposure: {fe}, Max Feature Exposure: {max_fe}, Square Sum: {square_sum_fe}\"\n",
    "    )\n",
    "    return spearman, payout, numerai_sharpe, mae\n",
    "\n",
    "\n",
    "spearman, payout, numerai_sharpe, mae = evaluate(validation, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-adolescent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
