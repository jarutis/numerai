{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "significant-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../src/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "sapphire-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "suspected-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = utils.read_current('../data/round_255/')\n",
    "train['era'] = train.era.str.extract('(\\d+|X)$', expand=False).str.zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "global-parks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test          1528119\n",
       "validation     137779\n",
       "live             5411\n",
       "Name: data_type, dtype: Int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.data_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "direct-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "floppy-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-staff",
   "metadata": {},
   "source": [
    "# Experimentas NR003\n",
    "\n",
    "Kaip sugrupuoti eras. Idėja klasterizuoti pagal koreliaciją tarp kintamųjų. Problema kad jei paėmus visus kintamuosius gausis $300^2/2$ variantų. Tiek daug dimensijų sunku suklasterizuoti. Reikia kažkaip sumažinti dimensijų skaičių. PCA nelabai veikė. Pabandom imti vidurkį tarp skirtingų grupių."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "strong-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLusterModel:\n",
    "    def __init__(self, data, features, era_aggregator, clusterer, model_factory):\n",
    "        self.agg = era_aggregator\n",
    "        self.clusterer = clusterer\n",
    "        self.model_factory = model_factory\n",
    "        self.data = data\n",
    "        \n",
    "    def cluster(self):\n",
    "        self.correlations = self.data.groupby('era').apply(self.agg)\n",
    "        self.era_clusters = self.clusterer.fit_predict(correlations)\n",
    "        self.labels = pd.Series(self.era_clusters).unique()\n",
    "        \n",
    "    def train(self):\n",
    "        self.models = {key:self.model_factory() for key in self.labels}\n",
    "        data = self.data[self.data.data_type == 'train']\n",
    "        for key, model in self.models.items():\n",
    "            data_group = data[data.era.isin(self.correlations.reset_index().era[self.era_clusters == key])].copy()\n",
    "            model.fit(data_group[features], data_group.target)\n",
    "            \n",
    "    def validate(self):\n",
    "        out = []\n",
    "        data = self.data[self.data.data_type == 'validation']\n",
    "        for key, model in self.models.items():\n",
    "            data_group = data[data.era.isin(self.correlations.reset_index().era[self.era_clusters == key])].copy()\n",
    "            data[\"prediction\"] = model.predict(data[features])\n",
    "            out.append(data)\n",
    "        val = pd.concat(out)\n",
    "        return evaluate(val, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "narrative-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_factory():\n",
    "    return LGBMRegressor(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ancient-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CLusterModel(full, features, get_correlations, KMeans(n_clusters=3), model_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "inclusive-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cellular-aside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ultimate-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/nmr/lib/python3.9/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    }
   ],
   "source": [
    "cc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "successful-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-180-f07513321a44>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"prediction\"] = model.predict(data[features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era\n",
      "era121    0.018341\n",
      "era122    0.014944\n",
      "era123    0.013762\n",
      "era124    0.002236\n",
      "era125    0.019051\n",
      "era126    0.015341\n",
      "era127    0.019056\n",
      "era128    0.038309\n",
      "era129    0.001941\n",
      "era130    0.044727\n",
      "era131    0.012312\n",
      "era132    0.060971\n",
      "era197    0.001336\n",
      "era198   -0.006004\n",
      "era199   -0.016551\n",
      "era200    0.011000\n",
      "era201    0.002800\n",
      "era202    0.007348\n",
      "era203    0.026638\n",
      "era204   -0.002631\n",
      "era205   -0.005570\n",
      "era206    0.039856\n",
      "era207    0.008605\n",
      "era208   -0.009667\n",
      "era209    0.012407\n",
      "era210   -0.019532\n",
      "era211   -0.031392\n",
      "era212    0.011675\n",
      "dtype: float64\n",
      "Spearman Correlation: 0.0104\n",
      "Average Payout: 0.052\n",
      "Sharpe Ratio: 0.5231\n",
      "Mean Absolute Error (MAE): 0.1556\n",
      "Max drawdown: 0.05856941609719213\n",
      "Feature exposure: 0.07659312627362276, Max Feature Exposure: 0.24179176802822683, Square Sum: 1.8343259464961021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0104, 0.052, 0.5231, 0.1556)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "numerous-possibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [f for f in full.columns if re.match('feature.+\\d', f)]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "sealed-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for group in [\"intelligence\", \"wisdom\", \"charisma\", \"dexterity\", \"strength\", \"constitution\"]:\n",
    "            cols = [col for col in df.columns if group in col]\n",
    "            df[f\"feature_{group}_mean\"] = df[cols].mean(axis=1)\n",
    "            df[f\"feature_{group}_std\"] = df[cols].std(axis=1)\n",
    "            df[f\"feature_{group}_skew\"] = df[cols].skew(axis=1)\n",
    "        return df\n",
    "\n",
    "full = get_group_stats(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "oriented-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlations(df):\n",
    "    df = df[[c for c in df.columns if 'mean' in c]]\n",
    "    corr_matrix = df.corr()\n",
    "    df_corr = corr_matrix.stack().reset_index()\n",
    "    df_corr.columns = ['FEATURE_1', 'FEATURE_2', 'CORRELATION']\n",
    "    mask_dups = (df_corr[['FEATURE_1', 'FEATURE_2']].apply(frozenset, axis=1).duplicated()) | (df_corr['FEATURE_1']==df_corr['FEATURE_2']) \n",
    "    df_corr = df_corr[~mask_dups]\n",
    "    return df_corr.CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "short-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = full.groupby('era').apply(get_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "silver-ending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>CORRELATION</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>era</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001</th>\n",
       "      <td>-0.261828</td>\n",
       "      <td>0.224596</td>\n",
       "      <td>0.357775</td>\n",
       "      <td>0.207713</td>\n",
       "      <td>-0.023901</td>\n",
       "      <td>-0.120759</td>\n",
       "      <td>-0.113478</td>\n",
       "      <td>-0.244795</td>\n",
       "      <td>0.094442</td>\n",
       "      <td>0.499943</td>\n",
       "      <td>0.358846</td>\n",
       "      <td>0.188486</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0.449269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002</th>\n",
       "      <td>-0.263603</td>\n",
       "      <td>0.230903</td>\n",
       "      <td>0.356329</td>\n",
       "      <td>0.223538</td>\n",
       "      <td>-0.028203</td>\n",
       "      <td>-0.160691</td>\n",
       "      <td>-0.160422</td>\n",
       "      <td>-0.280032</td>\n",
       "      <td>0.084017</td>\n",
       "      <td>0.536873</td>\n",
       "      <td>0.401641</td>\n",
       "      <td>0.202604</td>\n",
       "      <td>0.289249</td>\n",
       "      <td>0.172553</td>\n",
       "      <td>0.436288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003</th>\n",
       "      <td>-0.244919</td>\n",
       "      <td>0.211500</td>\n",
       "      <td>0.381217</td>\n",
       "      <td>0.217929</td>\n",
       "      <td>-0.031521</td>\n",
       "      <td>-0.177733</td>\n",
       "      <td>-0.188151</td>\n",
       "      <td>-0.286470</td>\n",
       "      <td>0.093519</td>\n",
       "      <td>0.546119</td>\n",
       "      <td>0.431544</td>\n",
       "      <td>0.232220</td>\n",
       "      <td>0.299271</td>\n",
       "      <td>0.164915</td>\n",
       "      <td>0.440053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004</th>\n",
       "      <td>-0.242308</td>\n",
       "      <td>0.220869</td>\n",
       "      <td>0.428731</td>\n",
       "      <td>0.231354</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.131175</td>\n",
       "      <td>-0.191416</td>\n",
       "      <td>-0.273598</td>\n",
       "      <td>0.104669</td>\n",
       "      <td>0.544824</td>\n",
       "      <td>0.442586</td>\n",
       "      <td>0.253280</td>\n",
       "      <td>0.306586</td>\n",
       "      <td>0.163820</td>\n",
       "      <td>0.425144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005</th>\n",
       "      <td>-0.270523</td>\n",
       "      <td>0.213598</td>\n",
       "      <td>0.490674</td>\n",
       "      <td>0.222092</td>\n",
       "      <td>-0.023230</td>\n",
       "      <td>-0.107948</td>\n",
       "      <td>-0.198479</td>\n",
       "      <td>-0.258038</td>\n",
       "      <td>0.134461</td>\n",
       "      <td>0.539266</td>\n",
       "      <td>0.434232</td>\n",
       "      <td>0.269917</td>\n",
       "      <td>0.287871</td>\n",
       "      <td>0.113592</td>\n",
       "      <td>0.437874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>era946</th>\n",
       "      <td>-0.332790</td>\n",
       "      <td>0.144782</td>\n",
       "      <td>0.616933</td>\n",
       "      <td>0.128513</td>\n",
       "      <td>-0.062108</td>\n",
       "      <td>0.114745</td>\n",
       "      <td>-0.215496</td>\n",
       "      <td>-0.043828</td>\n",
       "      <td>0.127156</td>\n",
       "      <td>0.391002</td>\n",
       "      <td>0.316447</td>\n",
       "      <td>0.192391</td>\n",
       "      <td>0.246393</td>\n",
       "      <td>0.057418</td>\n",
       "      <td>0.541553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>era947</th>\n",
       "      <td>-0.331398</td>\n",
       "      <td>0.141403</td>\n",
       "      <td>0.616277</td>\n",
       "      <td>0.132292</td>\n",
       "      <td>-0.063996</td>\n",
       "      <td>0.110464</td>\n",
       "      <td>-0.205904</td>\n",
       "      <td>-0.053859</td>\n",
       "      <td>0.135194</td>\n",
       "      <td>0.395125</td>\n",
       "      <td>0.328159</td>\n",
       "      <td>0.184938</td>\n",
       "      <td>0.242241</td>\n",
       "      <td>0.046950</td>\n",
       "      <td>0.532691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>era948</th>\n",
       "      <td>-0.322252</td>\n",
       "      <td>0.169005</td>\n",
       "      <td>0.618337</td>\n",
       "      <td>0.140119</td>\n",
       "      <td>-0.061699</td>\n",
       "      <td>0.080524</td>\n",
       "      <td>-0.168035</td>\n",
       "      <td>-0.064328</td>\n",
       "      <td>0.149849</td>\n",
       "      <td>0.418248</td>\n",
       "      <td>0.353738</td>\n",
       "      <td>0.176174</td>\n",
       "      <td>0.185376</td>\n",
       "      <td>-0.016323</td>\n",
       "      <td>0.524429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>era949</th>\n",
       "      <td>-0.298917</td>\n",
       "      <td>0.179455</td>\n",
       "      <td>0.621964</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>-0.070953</td>\n",
       "      <td>0.076793</td>\n",
       "      <td>-0.074735</td>\n",
       "      <td>-0.073326</td>\n",
       "      <td>0.167498</td>\n",
       "      <td>0.420415</td>\n",
       "      <td>0.360570</td>\n",
       "      <td>0.161499</td>\n",
       "      <td>0.114696</td>\n",
       "      <td>-0.084190</td>\n",
       "      <td>0.511555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eraX</th>\n",
       "      <td>-0.312547</td>\n",
       "      <td>0.184404</td>\n",
       "      <td>0.613322</td>\n",
       "      <td>0.133116</td>\n",
       "      <td>-0.107592</td>\n",
       "      <td>0.069044</td>\n",
       "      <td>-0.064685</td>\n",
       "      <td>-0.082192</td>\n",
       "      <td>0.177679</td>\n",
       "      <td>0.434448</td>\n",
       "      <td>0.369972</td>\n",
       "      <td>0.150231</td>\n",
       "      <td>0.125747</td>\n",
       "      <td>-0.096587</td>\n",
       "      <td>0.499281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "CORRELATION        1         2         3         4         5         8   \\\n",
       "era                                                                       \n",
       "0001        -0.261828  0.224596  0.357775  0.207713 -0.023901 -0.120759   \n",
       "0002        -0.263603  0.230903  0.356329  0.223538 -0.028203 -0.160691   \n",
       "0003        -0.244919  0.211500  0.381217  0.217929 -0.031521 -0.177733   \n",
       "0004        -0.242308  0.220869  0.428731  0.231354 -0.024993 -0.131175   \n",
       "0005        -0.270523  0.213598  0.490674  0.222092 -0.023230 -0.107948   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "era946      -0.332790  0.144782  0.616933  0.128513 -0.062108  0.114745   \n",
       "era947      -0.331398  0.141403  0.616277  0.132292 -0.063996  0.110464   \n",
       "era948      -0.322252  0.169005  0.618337  0.140119 -0.061699  0.080524   \n",
       "era949      -0.298917  0.179455  0.621964  0.138514 -0.070953  0.076793   \n",
       "eraX        -0.312547  0.184404  0.613322  0.133116 -0.107592  0.069044   \n",
       "\n",
       "CORRELATION        9         10        11        15        16        17  \\\n",
       "era                                                                       \n",
       "0001        -0.113478 -0.244795  0.094442  0.499943  0.358846  0.188486   \n",
       "0002        -0.160422 -0.280032  0.084017  0.536873  0.401641  0.202604   \n",
       "0003        -0.188151 -0.286470  0.093519  0.546119  0.431544  0.232220   \n",
       "0004        -0.191416 -0.273598  0.104669  0.544824  0.442586  0.253280   \n",
       "0005        -0.198479 -0.258038  0.134461  0.539266  0.434232  0.269917   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "era946      -0.215496 -0.043828  0.127156  0.391002  0.316447  0.192391   \n",
       "era947      -0.205904 -0.053859  0.135194  0.395125  0.328159  0.184938   \n",
       "era948      -0.168035 -0.064328  0.149849  0.418248  0.353738  0.176174   \n",
       "era949      -0.074735 -0.073326  0.167498  0.420415  0.360570  0.161499   \n",
       "eraX        -0.064685 -0.082192  0.177679  0.434448  0.369972  0.150231   \n",
       "\n",
       "CORRELATION        22        23        29  \n",
       "era                                        \n",
       "0001         0.249967  0.155829  0.449269  \n",
       "0002         0.289249  0.172553  0.436288  \n",
       "0003         0.299271  0.164915  0.440053  \n",
       "0004         0.306586  0.163820  0.425144  \n",
       "0005         0.287871  0.113592  0.437874  \n",
       "...               ...       ...       ...  \n",
       "era946       0.246393  0.057418  0.541553  \n",
       "era947       0.242241  0.046950  0.532691  \n",
       "era948       0.185376 -0.016323  0.524429  \n",
       "era949       0.114696 -0.084190  0.511555  \n",
       "eraX         0.125747 -0.096587  0.499281  \n",
       "\n",
       "[450 rows x 15 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "magnetic-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(metric='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "divine-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = clusterer.fit(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "coastal-gabriel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1,  1, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1,  8,  8,  8,  8,  8,  8,  8, -1,  8,\n",
       "       -1, -1,  8, -1, -1, -1, -1, -1, -1, -1,  4,  4,  4, -1,  4,  4, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  2, 13, 13, 13, 13, 13,\n",
       "       13, 13, 13, 13, 13, -1, 13, 13, 13, 13, 13, 13, 13, -1, -1, 17, -1,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 17, 16, -1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, -1, -1, -1, -1, -1,  9, -1, -1,  9, -1, -1,  9, -1, -1,  9, -1,\n",
       "       -1,  9,  9, -1,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, -1, -1, -1, -1, -1,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6, -1,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "       -1,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15, 15, -1, -1, -1, -1, -1, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, -1, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3, -1, -1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "middle-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = KMeans(n_clusters=2).fit(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "independent-governor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "numerical-performer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test          1528119\n",
       "train          501808\n",
       "validation     275558\n",
       "live             5411\n",
       "Name: data_type, dtype: Int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.data_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "greek-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full[full.data_type == 'train']\n",
    "validation = full[full.data_type == 'validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "piano-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_train = train.groupby('era').apply(get_correlations)\n",
    "train_labels = clustering.predict(correlations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "radio-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train[train.era.isin(correlations_train.reset_index().era[train_labels == 0])].copy()\n",
    "train2 = train[train.era.isin(correlations_train.reset_index().era[train_labels == 1])].copy()\n",
    "train3 = train[train.era.isin(correlations_train.reset_index().era[train_labels == 2])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "afraid-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"l2\",\n",
    "    \"num_iterations\": 2000,\n",
    "    \"learning_rate\": 0.006,\n",
    "    \"lambda_l1\": 1.4,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"bagging_fraction\": 0.55,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"num_leaves\": 107,\n",
    "    \"max_depth\": 15,\n",
    "    \"verbose\": 0,\n",
    "    \"random_state\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "apart-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/nmr/lib/python3.9/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.55, subsample=1.0 will be ignored. Current value: bagging_fraction=0.55\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.55, bagging_freq=1, lambda_l1=1.4,\n",
       "              lambda_l2=1.0, learning_rate=0.006, max_depth=15, metric='l2',\n",
       "              num_iterations=2000, num_leaves=107, objective='regression',\n",
       "              random_state=0, verbose=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "model1 = LGBMRegressor(**param)\n",
    "model1.fit(train1[features], train1.target)\n",
    "\n",
    "model2 = LGBMRegressor(**param)\n",
    "model2.fit(train2[features], train2.target)\n",
    "\n",
    "model3 = LGBMRegressor(**param)\n",
    "model3.fit(train3[features], train3.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "future-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_validation = validation.groupby('era').apply(get_correlations)\n",
    "validation_labels = clustering.predict(correlations_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "suitable-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = validation[validation.era.isin(correlations_validation.reset_index().era[validation_labels == 0])].copy()\n",
    "val2 = validation[validation.era.isin(correlations_validation.reset_index().era[validation_labels == 1])].copy()\n",
    "val3 = validation[validation.era.isin(correlations_validation.reset_index().era[validation_labels == 2])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "handy-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "val1[\"prediction\"] = model1.predict(val1[features])\n",
    "val2[\"prediction\"] = model2.predict(val2[features])\n",
    "val3[\"prediction\"] = model3.predict(val3[features])\n",
    "val = pd.concat([val1, val2, val3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ranging-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era\n",
      "era121   -0.018353\n",
      "era122    0.026287\n",
      "era123    0.049315\n",
      "era124    0.051818\n",
      "era125    0.018260\n",
      "era126    0.022001\n",
      "era127    0.009962\n",
      "era128    0.034088\n",
      "era129   -0.036523\n",
      "era130    0.044706\n",
      "era131    0.026467\n",
      "era132    0.055581\n",
      "era197    0.001118\n",
      "era198   -0.008374\n",
      "era199   -0.011497\n",
      "era200    0.004092\n",
      "era201   -0.008557\n",
      "era202    0.016605\n",
      "era203    0.009686\n",
      "era204    0.039270\n",
      "era205   -0.000015\n",
      "era206    0.044208\n",
      "era207   -0.002118\n",
      "era208    0.001970\n",
      "era209    0.026041\n",
      "era210   -0.012382\n",
      "era211   -0.026737\n",
      "era212    0.001656\n",
      "dtype: float64\n",
      "Spearman Correlation: 0.0128\n",
      "Average Payout: 0.064\n",
      "Sharpe Ratio: 0.5185\n",
      "Mean Absolute Error (MAE): 0.1555\n",
      "Max drawdown: 0.05085234446654363\n",
      "Feature exposure: 0.06719175458095991, Max Feature Exposure: 0.2462870347510918, Square Sum: 1.5390171873490497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def spearmanr(target, pred):\n",
    "    return np.corrcoef(target, pred.rank(pct=True, method=\"first\"))[0, 1]\n",
    "\n",
    "\n",
    "def sharpe_ratio(corrs: pd.Series) -> np.float32:\n",
    "    \"\"\"\n",
    "        Calculate the Sharpe ratio for Numerai by using grouped per-era data\n",
    "\n",
    "        :param corrs: A Pandas Series containing the Spearman correlations for each era\n",
    "        :return: A float denoting the Sharpe ratio of your predictions.\n",
    "        \"\"\"\n",
    "    return corrs.mean() / corrs.std()\n",
    "\n",
    "\n",
    "# https://parmarsuraj99.medium.com/evaluating-financial-machine-learning-models-on-numerai-3562da8fd90\n",
    "def calculate_feature_exposure(df, feature_names, prediction_name=\"prediction\") -> list:\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    -----\n",
    "    feature_exposure, max_feat_exposure, square_sum_feature_exposure = calculate_feature_exposure(df, feature_names)\n",
    "    \"\"\"\n",
    "\n",
    "    exposures = []\n",
    "    for feature_name in feature_names:\n",
    "        exposures.append(spearmanr(df[feature_name], df[prediction_name]))\n",
    "\n",
    "    max_feat_exposure = np.max(np.abs(exposures))\n",
    "    square_sum_feature_exposure = np.sum([e ** 2 for e in exposures])\n",
    "    feature_exposure = np.std(exposures)\n",
    "\n",
    "    return [feature_exposure, max_feat_exposure, square_sum_feature_exposure]\n",
    "\n",
    "\n",
    "# Calculating Max Drawdown\n",
    "def max_drawdown(df, prediction_name=\"prediction\", target_name=\"target\"):\n",
    "    scores_per_era = df.groupby(\"era\").apply(\n",
    "        lambda x: spearmanr(x[prediction_name], x[target_name])\n",
    "    )\n",
    "\n",
    "    rolling_max = (\n",
    "        (scores_per_era + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
    "    )\n",
    "    daily_value = (scores_per_era + 1).cumprod()\n",
    "    max_drawdown = (rolling_max - daily_value).max()\n",
    "\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def evaluate(df: pd.DataFrame, features) -> tuple:\n",
    "    \"\"\"\n",
    "        Evaluate and display relevant metrics for Numerai \n",
    "\n",
    "        :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and a column for predictions\n",
    "        :param pred_col: The column where the predictions are stored\n",
    "        :return: A tuple of float containing the metrics\n",
    "        \"\"\"\n",
    "\n",
    "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
    "        \"\"\"Calculates Spearman correlation\"\"\"\n",
    "        return spearmanr(sub_df[\"target\"], sub_df[\"prediction\"])\n",
    "\n",
    "    # Calculate metrics\n",
    "    corrs = df.groupby(\"era\").apply(_score)\n",
    "    print(corrs)\n",
    "    payout_raw = (corrs / 0.2).clip(-1, 1)\n",
    "    spearman = round(corrs.mean(), 4)\n",
    "\n",
    "    payout = round(payout_raw.mean(), 4)\n",
    "    numerai_sharpe = round(sharpe_ratio(corrs), 4)\n",
    "    mae = mean_absolute_error(df[\"target\"], df[\"prediction\"]).round(4)\n",
    "    drawdown = max_drawdown(df)\n",
    "    fe, max_fe, square_sum_fe = calculate_feature_exposure(df, features)\n",
    "\n",
    "    # Display metrics\n",
    "    print(f\"Spearman Correlation: {spearman}\")\n",
    "    print(f\"Average Payout: {payout}\")\n",
    "    print(f\"Sharpe Ratio: {numerai_sharpe}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Max drawdown: {drawdown}\")\n",
    "    print(\n",
    "        f\"Feature exposure: {fe}, Max Feature Exposure: {max_fe}, Square Sum: {square_sum_fe}\"\n",
    "    )\n",
    "    return spearman, payout, numerai_sharpe, mae\n",
    "\n",
    "\n",
    "spearman, payout, numerai_sharpe, mae = evaluate(val, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-thread",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
